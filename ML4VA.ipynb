{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML4VA.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "oQaK40Co90tp",
        "y62pERsRGr1i",
        "WpVkAy8_GgQB",
        "7MH9rUImUqFV",
        "2w0uKtLqXrQ_",
        "E5xu6TZ1GjwI",
        "ajvSgZX9vSTx",
        "cLU0Y6hW50rd",
        "YXwrx275Pkpx"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyytr0sc6HLY"
      },
      "source": [
        "\n",
        "# Machine Learning for VA: 911 Call Priority Classifier \n",
        "\n",
        "___\n",
        "\n",
        " Muhammad H. Sareini - mhs3vh@virginia.edu\n",
        " \n",
        "Javier Rosas Ruiz - jr2dj@virginia.edu\n",
        " \n",
        "Sammy R. Hecht - srh2kq@virginia.edu\n",
        " \n",
        " ---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMvNuMTJ55B8"
      },
      "source": [
        "# Some pagkages to import\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pandas.plotting import scatter_matrix # optional\n",
        "from sklearn.preprocessing import Imputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQaK40Co90tp"
      },
      "source": [
        "## Load the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4X_XlwS7u-n"
      },
      "source": [
        "calls = pd.read_csv(\"~/ml/Police_Calls_for_Service.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW1vnPyDkUqr"
      },
      "source": [
        "calls.head() # look at the data "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esbCl0tGkw2R"
      },
      "source": [
        "## Data Pre-Processing \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7bWHEeBqsel"
      },
      "source": [
        "### Use Only Relevant Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S26p2iGok1ii"
      },
      "source": [
        "## Drop unnecessary data columns we will not use \n",
        "\n",
        "columns_to_drop = ['Incident Number',\n",
        "                   'Report Number', \n",
        "                   'Subdivision', \n",
        "                   'Entry Date/Time', \n",
        "                   'Dispatch Date/Time', \n",
        "                   'En Route Date/Time', \n",
        "                   'On Scene Date/Time', \n",
        "                   'Close Date/Time']\n",
        "\n",
        "calls = calls.drop(columns_to_drop, axis=1)\n",
        "\n",
        "# only takes locations that have coordinates in them \n",
        "calls = calls[calls[\"Location\"].str.count(\"\\n\") == 2]\n",
        "\n",
        "# get rid of null values that will ruin pipeline in Case Disposition\n",
        "calls = calls.dropna(subset=[\"Case Disposition\"]) \n",
        "\n",
        "# Fix error where half of calls is ints, other half is str\n",
        "calls = calls.astype( {\"Zone\": str})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XkeGAoMgzAA"
      },
      "source": [
        "calls.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S4qZUJ0qctM"
      },
      "source": [
        "### Break Up Attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXTjT6IYfUxt"
      },
      "source": [
        "### an attrib addr pipeline only adds attributes, but I need \n",
        "### to break current attributes up so I am doing it manually \n",
        "\n",
        "# Break date/time into month, year, hour and convert to millitary time \n",
        "\n",
        "dt = calls[\"Call Date/Time\"]\n",
        "\n",
        "month = dt.str[:2]\n",
        "\n",
        "year = dt.str[6:10]\n",
        "\n",
        "def convert_millitary_time(dt):\n",
        "  \"\"\"\n",
        "  takes in datetime and returns the hour converted into millitary time \n",
        "  \"\"\"\n",
        "  \n",
        "  hr = dt[11:13]\n",
        "  am_pm = dt[-2:]\n",
        "  num_hr = int(hr)\n",
        "  \n",
        "  if am_pm == \"PM\":\n",
        "    return str(num_hr + 12)\n",
        "  else:\n",
        "    return hr \n",
        "  \n",
        "hr = dt.apply(convert_millitary_time) \n",
        "\n",
        "# Get Lat and Long \n",
        "def get_lat(loc):\n",
        "  \"\"\"\n",
        "  takes location string and returns lat and long \n",
        "  \"\"\"\n",
        "  tup = loc.split(\"\\n\")[-1]\n",
        "  tup = tup.replace(\"(\",\"\").replace(\")\",\"\")\n",
        "  lat,long = tup.split(\",\")\n",
        "  return float(lat)\n",
        "\n",
        "def get_long(loc):\n",
        "  \"\"\"\n",
        "  takes location string and returns lat and long \n",
        "  \"\"\"\n",
        "  tup = loc.split(\"\\n\")[-1]\n",
        "  tup = tup.replace(\"(\",\"\").replace(\")\",\"\")\n",
        "  lat,long = tup.split(\",\")\n",
        "  return float(long)\n",
        "\n",
        "lat = calls['Location'].apply(get_lat)\n",
        "long = calls['Location'].apply(get_long)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyP5JKxCr9EU"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.misc import imread\n",
        "import matplotlib.cbook as cbook\n",
        "\n",
        "\n",
        "# url for image is: https://cdn.theatlantic.com/media/old_wire/img/upload/2012/04/06/mayfairmews.04062012.png\n",
        "datafile = cbook.get_sample_data(\"c:\\\\users\\\\sammy hecht\\\\ml\\\\va-beach.png\")\n",
        "img = imread(datafile)\n",
        "plt.scatter(lat[:100], long[:100], color=\"red\")\n",
        "plt.imshow(img, zorder=0, extent=[min(lat[:100]), max(lat[:100]), min(long[:100]), max(long[:100])])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNLkkhTBTrIj"
      },
      "source": [
        "## Have a look at the new table \n",
        "calls['Month'] = month\n",
        "# calls['Year'] = year\n",
        "calls['Hour'] = hr\n",
        "calls['Lat'] = lat\n",
        "calls['Long'] = long\n",
        "calls = calls.drop([\"Call Date/Time\", \"Location\"], axis=1) # drop the unnecessary columns now \n",
        "\n",
        "## See how the data looks now \n",
        "calls.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejYXZXC5SsAI"
      },
      "source": [
        "corr_matrix = calls.corr() \n",
        "corr_matrix[\"Priority\"].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuFi8ltvxG-l"
      },
      "source": [
        "### Split Data into training and test sets \n",
        "\n",
        "\n",
        "train_set,test_set = train_test_split(calls, test_size=0.2, random_state=42)\n",
        "\n",
        "y_train = train_set['Priority']\n",
        "X_train = train_set.drop('Priority', axis=1)\n",
        "\n",
        "y_test = test_set['Priority']\n",
        "X_test = test_set.drop('Priority', axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UeDpK6DwCpA"
      },
      "source": [
        "### Pipeline \n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer \n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "\n",
        "num_attribs = [\"Lat\", \"Long\"]\n",
        "cat_attribs = [\"Call Type\", \"Zone\", \"Case Disposition\", \"Month\", \"Hour\"]\n",
        "\n",
        "# cats is the categories we will encode\n",
        "cats = [calls['Call Type'].unique(),calls['Zone'].unique(),calls['Case Disposition'].unique(),calls['Month'].unique(),calls['Hour'].unique()]\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        (\"cat\", OneHotEncoder(categories=cats), cat_attribs),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "X_test = full_pipeline.fit_transform(X_test)\n",
        "X_train = full_pipeline.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2QdyGgxyEk9"
      },
      "source": [
        "## Model Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y62pERsRGr1i"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFGGV65HGplU"
      },
      "source": [
        "from sklearn.metrics import recall_score, accuracy_score, f1_score, precision_score\n",
        "# Logistic Regression\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression(random_state=42, C=1)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "acc_sc = accuracy_score(log_reg.predict(X_test), y_test)\n",
        "prec_sc = precision_score(log_reg.predict(X_test), y_test, average=\"macro\")\n",
        "f1_sc = f1_score(log_reg.predict(X_test), y_test, average=\"macro\")\n",
        "rec_sc = recall_score(log_reg.predict(X_test), y_test, average=\"macro\")\n",
        "\n",
        "print(acc_sc, prec_sc, f1_sc, rec_sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceUM9g9eERKu"
      },
      "source": [
        "log_reg.predict(X_test[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMn7gmkNOz68"
      },
      "source": [
        "0.9699916666666667 \n",
        "\n",
        "0.6693597069097817 \n",
        "\n",
        "0.7015597844828797 \n",
        "\n",
        "0.9693108007538811\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpVkAy8_GgQB"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSK7gEORGjDs"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Training your svm here\n",
        "svm_clf = LinearSVC(C=1, loss=\"hinge\", random_state=42, max_iter=10)\n",
        "svm_clf.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Testing your svm here\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "y_train_predict = cross_val_predict(svm_clf, X_train, y_train, cv=3)\n",
        "\n",
        "\n",
        "# 1) Accuracy: \n",
        "print(\"Accuracy Score: \", np.average(cross_val_score(svm_clf, X_train, y_train, cv=3, scoring=\"accuracy\")))\n",
        "\n",
        "\n",
        "# 2) Precision: \n",
        "print(\"Precision Score: \", precision_score(y_train, y_train_predict, average=\"macro\"))\n",
        "\n",
        "\n",
        "# 3) Recall: \n",
        "print(\"Recall Score: \", recall_score(y_train, y_train_predict, average=\"macro\"))\n",
        "\n",
        "\n",
        "# 4) F-1 Score: \n",
        "print(\"F-1 Score: \", f1_score(y_train, y_train_predict, average=\"macro\"))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92lNaL8SPACg"
      },
      "source": [
        "Accuracy Score:  0.9689936222111335\n",
        "\n",
        "Precision Score:  0.920261552357195\n",
        "\n",
        "Recall Score:  0.6628074524914764\n",
        "\n",
        "F-1 Score:  0.6941962931810439"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MH9rUImUqFV"
      },
      "source": [
        "### Tuning SVM with the regularization hyperparameter C in order to avoid overfitting and get better accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhZP7XomVJ8x"
      },
      "source": [
        "# C = 1       \n",
        "svm_clf1 = LinearSVC(C=1, loss=\"hinge\", random_state=42, max_iter=10)\n",
        "svm_clf1.fit(X_train, y_train.ravel())\n",
        "score1 = np.average(cross_val_score(svm_clf1, X_train, y_train, cv=3, scoring=\"accuracy\"))    \n",
        "print(\"The accuracy when C = 1: \", score1)\n",
        "\n",
        "\n",
        "# C = 10\n",
        "svm_clf2 = LinearSVC(C=10, loss=\"hinge\", random_state=42, max_iter=10)\n",
        "svm_clf2.fit(X_train, y_train.ravel())\n",
        "score2 = np.average(cross_val_score(svm_clf2, X_train, y_train, cv=3, scoring=\"accuracy\"))  \n",
        "print(\"The accuracy when C = 10: \", score2)\n",
        "\n",
        "\n",
        "# C = 13 \n",
        "svm_clf3 = LinearSVC(C=13, loss=\"hinge\", random_state=42, max_iter=10)\n",
        "svm_clf3.fit(X_train, y_train.ravel())\n",
        "score3 = np.average(cross_val_score(svm_clf3, X_train, y_train, cv=3, scoring=\"accuracy\"))  \n",
        "print(\"The accuracy when C = 13: \", score3)\n",
        "\n",
        "\n",
        "# C = 20 \n",
        "svm_clf4 = LinearSVC(C=20, loss=\"hinge\", random_state=42, max_iter=10)\n",
        "svm_clf4.fit(X_train, y_train.ravel())\n",
        "score4 = np.average(cross_val_score(svm_clf4, X_train, y_train, cv=3, scoring=\"accuracy\"))  \n",
        "print(\"The accuracy when C = 20: \", score4)\n",
        "\n",
        "\n",
        "# C = 50\n",
        "svm_clf5 = LinearSVC(C=50, loss=\"hinge\", random_state=42, max_iter=10)\n",
        "svm_clf5.fit(X_train, y_train.ravel())\n",
        "score5 = np.average(cross_val_score(svm_clf5, X_train, y_train, cv=3, scoring=\"accuracy\"))  \n",
        "print(\"The accuracy when C = 50: \", score5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1JEsVSmXJxz"
      },
      "source": [
        "print(\"Best value of C is clearly C = 1, with an accuracy of 0.9691505519240343\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w0uKtLqXrQ_"
      },
      "source": [
        "### **DO NOT RUN THE CELL BELOW.** DUE TO THE SIZE OF OUR DATASET, IT TAKES ABOUT 1 HOUR TO RUN. The cell is Kernelizing the SVM with the Gaussian RBF. RESULTS OF THE GAUSSIAN KERNEL ARE WRITTEN BELOW the code cell. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7VASdClX4zZ"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "\n",
        "rbf_kernel_svm_clf1 = SVC(kernel=\"rbf\", gamma=0.1, C=1)\n",
        "rbf_kernel_svm_clf1.fit(X_train, y_train)\n",
        "y_train_predict1 = cross_val_predict(rbf_kernel_svm_clf1, X_train, y_train, cv=3)\n",
        "\n",
        "\n",
        "# 1) Accuracy: \n",
        "print(\"Accuracy Score: \", np.average(cross_val_score(svm_clf, X_train, y_train, cv=3, scoring=\"accuracy\")))\n",
        "\n",
        "\n",
        "# 2) Precision: \n",
        "print(\"Precision Score: \", precision_score(y_train, y_train_predict1))\n",
        "\n",
        "\n",
        "# 3) Recall: \n",
        "print(\"Recall Score: \", recall_score(y_train, y_train_predict1))\n",
        "\n",
        "\n",
        "# 4) F-1 Score: \n",
        "print(\"F-1 Score: \", f1_score(y_train, y_train_predict1))\n",
        "\n",
        "\n",
        "# Creating ROC curve: \n",
        "y_scores1 = cross_val_predict(rbf_kernel_svm_clf1, X_train, y_train, cv=3, method=\"decision_function\")\n",
        "fpr1, tpr1, thresholds1 = roc_curve(y_train, y_scores1)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, linewidth=2, label=None)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.axis([0, 1, 0, 1])\n",
        "plt.xlabel('False Positive Rate', fontsize=16)\n",
        "plt.ylabel('True Positive Rate', fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNcZPlhvkkdd"
      },
      "source": [
        "Accuracy Score: 0.92\n",
        "\n",
        "Precision Score: 0.89\n",
        "\n",
        "Recall Score: 0.71\n",
        "\n",
        "F-1 Score: 0.60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5xu6TZ1GjwI"
      },
      "source": [
        "### Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB2zn0aazV0m"
      },
      "source": [
        "Now, decision tree has no minimum depth. By increasing the minimum depth from 2,  the scores increased from the following:\n",
        "\n",
        ">**Accuracy: 0.7848583333333333**\n",
        "\n",
        ">**Precision: 0.5412268928390945**\n",
        "\n",
        ">**Recall: 0.30713560243964166**\n",
        "\n",
        ">**F1 Score: 0.3405258278733306**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2g_4a-ByN5v"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "\n",
        "d_tree = DecisionTreeClassifier(random_state=42, max_depth=70)\n",
        "d_tree.fit(X_train, y_train)\n",
        "\n",
        "y_pred_d_tree = d_tree.predict(X_test) # predictions \n",
        "\n",
        "## accuracy\n",
        "acc_score = accuracy_score(y_test, y_pred_d_tree)\n",
        "print(\"Accuracy:\", acc_score)\n",
        "\n",
        "## precision \n",
        "prec_score = precision_score(y_test, y_pred_d_tree, average='macro')\n",
        "print(\"Precision:\", prec_score)\n",
        "\n",
        "## recall\n",
        "rec_score = recall_score(y_test, y_pred_d_tree, average='macro')\n",
        "print(\"Recall:\", rec_score) \n",
        "\n",
        "## F1-Score\n",
        "f1 = f1_score(y_test, y_pred_d_tree, average='macro')\n",
        "print(\"F1 Score:\", f1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX7dyefaPJyO"
      },
      "source": [
        "*Accuracy*: 0.9610583333333333\n",
        "\n",
        "Precision: 0.7716911449224426\n",
        "\n",
        "Recall: 0.7358102746749348\n",
        "\n",
        "F1 Score: 0.7516427398957578"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajvSgZX9vSTx"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm56SWLM4gdm"
      },
      "source": [
        "Using random forest, precision went up over 10% with a small sacrifice to recall and F1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB7ljBoPvTp_"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "r_forest = RandomForestClassifier(n_estimators=10, max_depth=70, random_state=42)\n",
        "r_forest.fit(X_train, y_train)\n",
        "y_pred_r_forest = r_forest.predict(X_test)\n",
        "\n",
        "## accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_r_forest))\n",
        "\n",
        "## precision \n",
        "print(\"Precision:\",precision_score(y_test,y_pred_r_forest, average='macro'))\n",
        "\n",
        "## recall\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_r_forest, average='macro')) \n",
        "\n",
        "## F1-Score\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred_r_forest, average='macro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtJOwr3_PXjX"
      },
      "source": [
        "Accuracy: 0.9685416666666666\n",
        "\n",
        "Precision: 0.8939483268758561\n",
        "\n",
        "Recall: 0.6959151692412575\n",
        "\n",
        "F1 Score: 0.7421529437200082"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLU0Y6hW50rd"
      },
      "source": [
        "### Ensemble Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMpeLYXY54Ap"
      },
      "source": [
        "We will use an ensemble voting algorithm on each classifier we have already trained, to see if we can get a better performance. After playing with the parameters, it was found that after dropping random forest or decision tree classifier, the algorithm performs better in precision with only a small hit to F1 and recall resulting in very high accuracy and precision. In the end we decided to drop the random forest in favor of the decision tree classifier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THPN2vB57nU0"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# use all previously named classifiers \n",
        "named_classifiers = [\n",
        "    ('log_reg', log_reg),\n",
        "    ('svm_clf', svm_clf),\n",
        "    ('d_tree', d_tree),\n",
        "]\n",
        "\n",
        "# train the model \n",
        "voting_clf = VotingClassifier(named_classifiers)\n",
        "voting_clf.fit(X_train,y_train)\n",
        "\n",
        "# predict the model \n",
        "y_pred_ensemble = voting_clf.predict(X_test)\n",
        "\n",
        "## accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_ensemble))\n",
        "\n",
        "## precision \n",
        "print(\"Precision:\",precision_score(y_test,y_pred_ensemble, average='macro'))\n",
        "\n",
        "## recall\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_ensemble, average='macro')) \n",
        "\n",
        "## F1-Score\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred_ensemble, average='macro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edAHNavlP9nr"
      },
      "source": [
        "Accuracy: 0.969925\n",
        "\n",
        "Precision: 0.9703112433006822\n",
        "\n",
        "Recall: 0.6685719583226668\n",
        "\n",
        "F1 Score: 0.7009708502651231"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXwrx275Pkpx"
      },
      "source": [
        "### Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tirImIx48Q7p"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# train the boosting model\n",
        "grad_boost = GradientBoostingClassifier(loss=\"deviance\", learning_rate=.2, random_state=42, n_estimators=50)\n",
        "\n",
        "grad_boost.fit(X_train, y_train)\n",
        "\n",
        "# predict the model\n",
        "y_pred_boost = grad_boost.predict(X_test)\n",
        "\n",
        "\n",
        "## accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_boost))\n",
        "\n",
        "## precision \n",
        "print(\"Precision:\",precision_score(y_test,y_pred_boost, average='macro'))\n",
        "\n",
        "## recall\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_boost, average='macro')) \n",
        "\n",
        "## F1-Score\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred_boost, average='macro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edhOcpU0xzJ0"
      },
      "source": [
        "The GradientBoostingClassifier performed well in terms of accuracy, recall, and F1 score, but suffered slightly with precision.  With a learning rate of .1, and 100 boosting stages, the metrics were as follows:\n",
        "\n",
        ">Accuracy: 0.9648581844635313\n",
        "\n",
        ">Precision: 0.8866753181067324\n",
        "\n",
        ">Recall: 0.7310775582409732\n",
        "\n",
        ">F1 Score: 0.7759839794883954"
      ]
    }
  ]
}